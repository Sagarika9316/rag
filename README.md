In this guide, we will learn how to:

ðŸ’» Develop a retrieval augmented generation (RAG) based LLM application from scratch.
ðŸš€ Scale the major workloads (load, chunk, embed, index, serve, etc.) across multiple workers with different compute resources.
âœ… Evaluate different configurations of our application to optimize for both per-component (ex. retrieval_score) and overall performance (quality_score).
ðŸ”€ Implement a hybrid agent routing approach b/w OSS and closed LLMs to create the most performant and cost effective application.
ðŸ“¦ Serve the application in a highly scalable and available manner.
ðŸ’¡ Learn how methods like fine-tuning, prompt engineering, lexical search, reranking, data flywheel, etc. impact our application's performance.
